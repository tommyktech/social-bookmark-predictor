{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bookmark-regressor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkXB53-whq-G",
        "outputId": "4ebcae5b-5a5f-485e-a409-b368eb531ada"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install beautifulsoup4"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlMCCDOkoJe3",
        "outputId": "30fe1779-63dd-438f-86dd-5d33653c17c3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIMBwP6aDQ_w",
        "outputId": "e06bb3b3-15f9-48f5-a9eb-73614251d9fb"
      },
      "source": [
        "# ブックマークサイトからデータを集める\n",
        "import requests, datetime, json, time\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "def collect_entries(html):\n",
        "    soup = BeautifulSoup(html)\n",
        "    elems = soup.select(\".entrylist-contents\")\n",
        "    entries = []\n",
        "    for elem in elems:\n",
        "        title = elem.select(\"a.js-keyboard-openable\")[0].get(\"title\")\n",
        "        href = elem.select(\"a.js-keyboard-openable\")[0].get(\"href\")\n",
        "        count = elem.select(\"a.js-keyboard-entry-page-openable span\")[0].text\n",
        "        categ = elem.select(\"li.entrylist-contents-category\")[0].text\n",
        "        desc = elem.select(\"p.entrylist-contents-description\")[0].text\n",
        "        domain = urlparse(href).netloc\n",
        "        entries.append({\"b_count\": count, \"title\": title, \"desc\": desc, \"domain\": domain})\n",
        "    return entries\n",
        "\n",
        "def collect_hatebu_articles(genre=\"all\", limit=1000):\n",
        "    base_url = \"https://b.hatena.ne.jp/hotentry/{}/\".format(genre)\n",
        "    now = int(time.time())\n",
        "    now = now - 2 * 86400\n",
        "\n",
        "    entries = []\n",
        "    for i in range(limit):\n",
        "        now = now - 86400\n",
        "        dt = datetime.datetime.fromtimestamp(now)\n",
        "        date_str = str(dt.strftime(\"%Y%m%d\"))\n",
        "        print(date_str)\n",
        "        url = base_url + date_str\n",
        "        res = requests.get(url)\n",
        "        entries.extend(collect_entries(res.text))\n",
        "    pd.DataFrame(entries).to_csv(\"all_hatebu_articles_{}.csv\".format(genre), header=True, index=True)\n",
        "    return entries\n",
        "\n",
        "genres = [\"all\", \"general\", \"social\", \"economics\", \"life\", \"knowledge\", \"it\", \"fun\", \"entertainment\", \"game\"]\n",
        "for genre in genres:\n",
        "    collect_hatebu_articles(genre, limit=10)\n",
        "    break"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20210326\n",
            "20210325\n",
            "20210324\n",
            "20210323\n",
            "20210322\n",
            "20210321\n",
            "20210320\n",
            "20210319\n",
            "20210318\n",
            "20210317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz4dGhgEhDjd"
      },
      "source": [
        "# ファイルからデータをロードする\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "data_arr = []\n",
        "genres = [\"general\", \"social\", \"economics\", \"life\", \"knowledge\", \"it\", \"fun\", \"entertainment\", \"game\"]\n",
        "# genres = [\"all\"]\n",
        "for genre in genres:\n",
        "    cnt = 0\n",
        "    file_name = \"drive/MyDrive/all_hatebu_articles_{}.csv\".format(genre)\n",
        "    df = pd.read_csv(file_name, index_col=0, header=0, encoding=\"utf-8\")\n",
        "    bulk_str = \"\"\n",
        "    for row in df.iterrows():\n",
        "        cnt += 1\n",
        "        if row[1]['desc'] is np.nan or row[1]['title'] is np.nan:\n",
        "            continue\n",
        "        data_arr.append({\"b_count\": row[1]['b_count'], \"date\": row[1]['date'], \"timestamp\": row[1]['timestamp'], \"domain\":row[1]['domain'], \"text\": \"{} {} {} {}\".format(genre, row[1]['domain'], row[1]['title'], row[1]['desc'])})\n",
        "\n",
        "df = pd.DataFrame(data_arr)\n",
        "# ブコメ数\n",
        "df['b_count_log'] = np.log(df['b_count'])\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKRcBdKGEJEv",
        "outputId": "28a1e52a-75ae-46b2-fd0c-cf76d1310e1b"
      },
      "source": [
        "# 時系列データとして扱う。ブコメ数 > 10のフィルタは、ブコメ数が10に達したもののみを予測対象にする、という実運用を想定\n",
        "X_test = df.loc[(df.date > 20210300)&(df.b_count > 10)].sample(frac=1.0, random_state=42)\n",
        "X_valid = df.loc[(df.date < 20210300) & (df.date > 20210200)&(df.b_count > 10)].sample(frac=1.0, random_state=42)\n",
        "X_train = df.loc[(df.date < 20210200)&(df.date > 20201000)&(df.b_count > 10)].sample(frac=1.0, random_state=42)\n",
        "X_train.shape, X_valid.shape, X_test.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34919, 6), (7954, 6), (6880, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK2-9LNBhlXw",
        "outputId": "96f1aef1-0adc-47b6-b46b-058d565fd7fc"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# OOM対策\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
        "else:\n",
        "    print(\"Not enough GPU hardware devices available\")\n",
        "\n",
        "max_seq_length = 256\n",
        "\n",
        "bert_folder = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_folder)\n",
        "\n",
        "# Tokenizerで文章をトークン化する\n",
        "def encode_sentence(s, tokenizer):\n",
        "    s = str(s)\n",
        "    tokens = list(tokenizer.tokenize(s))\n",
        "    tokens.append('[SEP]')\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# input_ids, attention_mask, token_type_ids を作るやつ\n",
        "def bert_encode(sentences, tokenizer):\n",
        "    tokenized_sentences = tf.ragged.constant([\n",
        "        encode_sentence(s, tokenizer)[:max_seq_length-1]\n",
        "        for s in sentences])\n",
        "\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*tokenized_sentences.shape[0]\n",
        "    \n",
        "    input_word_ids = tf.concat([cls, tokenized_sentences], axis=-1)\n",
        "    attention_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "    type_cls = tf.zeros_like(cls)\n",
        "    type_s1 = tf.zeros_like(tokenized_sentences)\n",
        "    token_type_ids = tf.concat(\n",
        "        [type_cls, type_s1], axis=-1).to_tensor()\n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_word_ids.to_tensor(),\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask}\n",
        "\n",
        "    return inputs\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') memory growth: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZIkdwavhlck"
      },
      "source": [
        "# Inputのテキストをエンコードする。あとlabel作る。\n",
        "train_dict = bert_encode(X_train['text'].values, tokenizer)\n",
        "valid_dict = bert_encode(X_valid['text'].values, tokenizer)\n",
        "test_dict  = bert_encode(X_test['text'].values,  tokenizer)\n",
        "\n",
        "train_log_labels  = np.array(X_train['b_count_log'], dtype=np.float32)\n",
        "valid_log_labels  = np.array(X_valid['b_count_log'], dtype=np.float32)\n",
        "test_log_labels   = np.array(X_test['b_count_log'],  dtype=np.float32)\n",
        "\n",
        "batch_size = 20\n",
        "train_log_dataset_batched = tf.data.Dataset.from_tensor_slices((train_dict, train_log_labels)).shuffle(10000).batch(batch_size)\n",
        "valid_log_dataset_batched = tf.data.Dataset.from_tensor_slices((valid_dict, valid_log_labels)).batch(batch_size)\n",
        "test_log_dataset_batched  = tf.data.Dataset.from_tensor_slices((test_dict,  test_log_labels)).batch(batch_size)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5FzFa2ghlfu",
        "outputId": "ac19a73d-560e-45a3-dc24-9d985a84e77b"
      },
      "source": [
        "# モデルを用意してTrainする\n",
        "from transformers import TFBertForSequenceClassification\n",
        "bert_folder = \"cl-tohoku/bert-base-japanese\"\n",
        "reg_model = TFBertForSequenceClassification.from_pretrained(bert_folder, from_pt=True, num_labels=1)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "reg_model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "hist = reg_model.fit(train_log_dataset_batched, validation_data=valid_log_dataset_batched, epochs=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f14ad6d7ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f14ad6d7ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f14c8f87c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f14c8f87c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1746/1746 [==============================] - ETA: 0s - loss: 1.3634WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1746/1746 [==============================] - 1213s 668ms/step - loss: 1.3632 - val_loss: 0.8494\n",
            "Epoch 2/2\n",
            "1746/1746 [==============================] - 1163s 666ms/step - loss: 0.8688 - val_loss: 0.8148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c56rLbpjZHM"
      },
      "source": [
        "reg_model.save_weights(\"pretrained_weights.h5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "O4GxUy0uSGIL",
        "outputId": "2ba87fc0-45c0-4d2e-87cb-4a44afea0702"
      },
      "source": [
        "# test データでモデルの出来を確認\n",
        "iterator = iter(test_log_dataset_batched)\n",
        "test_batch = next(iterator)\n",
        "res = reg_model.predict(test_batch)\n",
        "num1 = batch_size * 0\n",
        "num2 = num1 + batch_size\n",
        "pd.DataFrame({\"true\" : np.round(np.exp(test_batch[1].numpy())).astype(int),\n",
        "              \"predict\" : np.floor(np.exp(np.squeeze(res[0]))).astype(int),\n",
        "              \"text\": X_test[[\"b_count\", \"text\"]].iloc[num1:num2][\"text\"].values})\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>true</th>\n",
              "      <th>predict</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>91</td>\n",
              "      <td>it uit-inside.linecorp.com ep.79 「フォントのなかの人」と見...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13</td>\n",
              "      <td>43</td>\n",
              "      <td>game news.denfaminicogamer.jp 発売中止となったホラーゲーム『還...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>813</td>\n",
              "      <td>297</td>\n",
              "      <td>general www.ajimatics.com 「2乗してはじめて0になる数」とかあった...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>123</td>\n",
              "      <td>168</td>\n",
              "      <td>life card-media.money.rakuten.co.jp ラクに＆ちょっといい...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>42</td>\n",
              "      <td>game twitter.com ㍃ﾊｶｾ on Twitter: \"それはマジでやめろ 「...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>304</td>\n",
              "      <td>74</td>\n",
              "      <td>game anond.hatelabo.jp ブスが活躍する漫画が見たい 最初非モテ(男)が...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>83</td>\n",
              "      <td>213</td>\n",
              "      <td>general www.itmedia.co.jp アイリスオーヤマ初のノートPC登場　税別...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>24</td>\n",
              "      <td>114</td>\n",
              "      <td>game togetter.com 「初心者には明確な答えを、上級者には多彩な選択肢を」ゲー...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>823</td>\n",
              "      <td>409</td>\n",
              "      <td>general anond.hatelabo.jp 家賃保証会社の問題と解決策の検討 〜 天...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>166</td>\n",
              "      <td>88</td>\n",
              "      <td>game anond.hatelabo.jp やっぱりウマ娘の記事がステマにしか見えん ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>360</td>\n",
              "      <td>283</td>\n",
              "      <td>general news.yahoo.co.jp ひろゆきこと西村博之氏「一般的に読めない名...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>337</td>\n",
              "      <td>416</td>\n",
              "      <td>general anond.hatelabo.jp 「キモイ」男の隣の席に座りたくない女が涙...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>58</td>\n",
              "      <td>30</td>\n",
              "      <td>economics www.cnn.co.jp CNN.co.jp : 宇宙でエネルギーを集...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>317</td>\n",
              "      <td>341</td>\n",
              "      <td>general wezz-y.com 「日本は世界で◯◯位」にダマされないために　女性差別に...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>26</td>\n",
              "      <td>94</td>\n",
              "      <td>social www.asahi.com 全入国者に位置確認アプリ　日本政府、陰性証明を必須...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>economics www.bbc.com 公の場で顔を覆う服装を禁止、スイス国民投票で僅差...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>104</td>\n",
              "      <td>68</td>\n",
              "      <td>economics www.asahi.com 河井克行被告が議員辞職を表明　法廷で一転、買...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>442</td>\n",
              "      <td>413</td>\n",
              "      <td>general togetter.com 呉座勇一先生による「女性」や「他者」への発言・批判...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>67</td>\n",
              "      <td>65</td>\n",
              "      <td>it www.suzukikenichi.com Google、動画SEOのベストプラクティ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>knowledge twitter.com 本屋ユカイ on Twitter: \"講談社さん...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    true  predict                                               text\n",
              "0     16       91  it uit-inside.linecorp.com ep.79 「フォントのなかの人」と見...\n",
              "1     13       43  game news.denfaminicogamer.jp 発売中止となったホラーゲーム『還...\n",
              "2    813      297  general www.ajimatics.com 「2乗してはじめて0になる数」とかあった...\n",
              "3    123      168  life card-media.money.rakuten.co.jp ラクに＆ちょっといい...\n",
              "4     18       42  game twitter.com ㍃ﾊｶｾ on Twitter: \"それはマジでやめろ 「...\n",
              "5    304       74  game anond.hatelabo.jp ブスが活躍する漫画が見たい 最初非モテ(男)が...\n",
              "6     83      213  general www.itmedia.co.jp アイリスオーヤマ初のノートPC登場　税別...\n",
              "7     24      114  game togetter.com 「初心者には明確な答えを、上級者には多彩な選択肢を」ゲー...\n",
              "8    823      409  general anond.hatelabo.jp 家賃保証会社の問題と解決策の検討 〜 天...\n",
              "9    166       88  game anond.hatelabo.jp やっぱりウマ娘の記事がステマにしか見えん ht...\n",
              "10   360      283  general news.yahoo.co.jp ひろゆきこと西村博之氏「一般的に読めない名...\n",
              "11   337      416  general anond.hatelabo.jp 「キモイ」男の隣の席に座りたくない女が涙...\n",
              "12    58       30  economics www.cnn.co.jp CNN.co.jp : 宇宙でエネルギーを集...\n",
              "13   317      341  general wezz-y.com 「日本は世界で◯◯位」にダマされないために　女性差別に...\n",
              "14    26       94  social www.asahi.com 全入国者に位置確認アプリ　日本政府、陰性証明を必須...\n",
              "15    28       45  economics www.bbc.com 公の場で顔を覆う服装を禁止、スイス国民投票で僅差...\n",
              "16   104       68  economics www.asahi.com 河井克行被告が議員辞職を表明　法廷で一転、買...\n",
              "17   442      413  general togetter.com 呉座勇一先生による「女性」や「他者」への発言・批判...\n",
              "18    67       65  it www.suzukikenichi.com Google、動画SEOのベストプラクティ...\n",
              "19    36       48  knowledge twitter.com 本屋ユカイ on Twitter: \"講談社さん..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcyOuaOLjqnm",
        "outputId": "1270b0b9-2409-44b5-ad13-451cc8bcb17c"
      },
      "source": [
        "# 以下、既存のweightsを読み込むやつ\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification\n",
        "bert_folder = \"cl-tohoku/bert-base-japanese\"\n",
        "reg_model = TFBertForSequenceClassification.from_pretrained(bert_folder, from_pt=True, num_labels=1)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "reg_model.compile(optimizer=optimizer, loss=loss)\n",
        "reg_model.load_weights(\"drive/MyDrive/hatebu_regressor.h5\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}